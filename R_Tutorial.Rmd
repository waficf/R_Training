
> https://discuss.analyticsvidhya.com/t/what-are-the-packages-required-to-plot-a-fancy-rpart-plot-in-r/6776
> http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html
> http://www.cmap.polytechnique.fr/~lepennec/R/Learning/Learning.html
> http://michael.hahsler.net/SMU/EMIS7332/R/viz_classifier.html
# https://discuss.analyticsvidhya.com/t/what-are-the-packages-required-to-plot-a-fancy-rpart-plot-in-r/6776/2
# https://www.statmethods.net/advstats/cart.html


```{r}
#library(rattle) # Fancy tree
library(dplyr)
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
library(tm) # text mining in R
library(knitr) # Convert to html
library(SnowballC) # Removes stemming words
library(wordcloud)
library(tidyverse)
library(e1071) # Naive Bayes package
library(gmodels) # Uses crosstab function
library(C50) # Package that includes the decision tree algorithm
library(kernlab)
library(arules) # Apriori Algorithm and sparce reader
library(stats)
library(caret) # Splitting data including cross validation
library(ROCR) # ROC Curve
library(vcd) # Kappa
library(irr)
library(ipred) # Bagging 
library(adabag) # ADA BOOST
library(randomForest) # RANDOM FOREST
library(gbm) #Gradient Boost
```

```{r}
# On vectors before the below

# Poker and roulette winnings from Monday to Friday:
poker_vector <- c(140, -50, 20, -120, 240)
roulette_vector <- c(-24, -50, 100, -350, 10)
days_vector <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
names(poker_vector) <- days_vector
names(roulette_vector) <- days_vector

# Assign to total_daily how much you won/lost on each day
total_daily <- (poker_vector+roulette_vector)
total_daily

######################################VECTORS#############################################

# Poker and roulette winnings from Monday to Friday:
poker_vector <- c(140, -50, 20, -120, 240)
roulette_vector <- c(-24, -50, 100, -350, 10)
days_vector <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
names(poker_vector) <- days_vector
names(roulette_vector) <- days_vector

# Which days did you make money on poker?
selection_vector <- poker_vector > 0

# Select from poker_vector these days
poker_winning_days <- poker_vector[selection_vector]
poker_winning_days

#######################################DATFRAMES#############################################

# The planets_df data frame from the previous exercise is pre-loaded

# Print out diameter of Mercury (row 1, column 3)
#planets_df[1,3]

# Print out data for Mars (entire fourth row)
#planets_df[4,]

# Select first 5 values of diameter column
#planets_df[1:5, 'diameter']

# Adapt the code to select all columns for planets with rings
#planets_df[rings_vector, ]


```



# K-NN ALGORITHM FOR CLASSIFICATION PROBLEM

```{r}
wbc <- read.csv('/Users/wafic/Downloads/Machine-Learning-with-R-datasets-master/wisc_bc_data.csv')
```


```{r}
str(wbc)
```
```{r}
wbc <- wbc[-1]
```

```{r}

```

```{r}
table(wbc$diagnosis)
```

```{r}
wbc$diagnosis <- factor(wbc$diagnosis, levels = c('B', 'M'), labels = c('Bening', 'Malignant'))
```

```{r}
round(prop.table(table(wbc$diagnosis))*100, 1)
```

```{r}
summary(wbc[c('radius_mean', 'area_mean', 'smoothness_mean')])
```

```{r}
normalize <- function(x) {
  return (x - min(x)) / (max(x) - min(x))
}
```


```{r}
wbc_n <- as.data.frame(lapply(wbc[2:31], normalize))
```

```{r}
summary(wbc_n[c('radius_mean', 'area_mean', 'smoothness_mean')])
```

```{r}
wbc_train <- wbc_n[1:469,]
wbc_test <- wbc_n[470:569,]

wbc_train_labels <- wbc[1:469, 1]
wbc_test_labels <- wbc[470:569, 1]
```


```{r}
p <- knn(train = wbc_train, test = wbc_test, cl = wbc_train_labels, k=3)
```

```{r}
CrossTable(x=wbc_test_labels, y=p, prop.chisq = FALSE)
```


```{r}
# Scaling extreme values using the z-score
wbc_z <- as.data.frame(scale(wbc[-1]))
summary(wbc_z$area_mean)
```

```{r}
wbc_train <- wbc_z[1:469,]
wbc_test <- wbc_z[470:569,]

wbc_train_labels <- wbc[1:469, 1]
wbc_test_labels <- wbc[470:569, 1]
```

```{r}
p<- knn(train = wbc_train, test = wbc_test, cl= wbc_train_labels, k=5)
CrossTable(x=p, y=wbc_test_labels, prop.chisq = FALSE)
```


> NAIVE BAYES ALGORITHM FOR TEXT PROCESSING


```{r}
sms_raw <- read.csv('/Users/wafic/Downloads/Machine-Learning-with-R-datasets-master/sms_spam.csv', stringsAsFactors = FALSE)
```

```{r}
str(sms_raw)
```

```{r}
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
```

```{r}
table(sms_raw$type)
```

```{r}
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
```

```{r}
inspect(sms_corpus[1:2])
```

```{r}
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
```


```{r}
# instead of the above we can use the below function
replace_punctuation <- function(x){
  gsub("[[:punct:]]+", " ", x)
}
```


```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
```

```{r}
as.character(sms_corpus_clean[[110]])
```

```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
```


```{r}
# This is the sparce matrix and it includes 6,500 terms
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm
```

```{r}
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5559,]

sms_train_labels <- sms_raw[1:4169,]$type
sms_test_labels <- sms_raw[4170:5559,]$type
```

```{r}
prop.table(table(sms_test_labels))
```

```{r}
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
```

```{r}
# The final step in the data preparation process is to transform the sparse matrix into a data structure that can be used to train a Naive Bayes classi er. Currently, the sparse matrix includes over 6,500 features; this is a feature for every word that appears in at least one SMS message. It's unlikely that all of these are useful for classi cation. To reduce the number of features, we will eliminate any word that appear in less than  ve SMS messages, or in less than about 0.1 percent of the records in the training data

sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
```

```{r}
sms_dtm_freq_train <- sms_dtm_train[, sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[, sms_freq_words]
str(sms_dtm_freq_train)
```

```{r}
convert_counts <- function(x){
  x <- ifelse(x > 0, 'Yes', 'No')
}
```

```{r}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)

```

```{r}
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
```

```{r}
sms_test_pred <- predict(sms_classifier, sms_test)
```

```{r}
CrossTable(sms_test_pred, sms_test_labels, 
           prop.chisq = FALSE, prop.t = FALSE,
           dnn = c('predicted', 'actual'))
```

# DECISION TREE ALGORITHM FOR CREDIT APPROVAL PROCESS

```{r}
credit <- read.csv('/Users/wafic/Downloads/Machine-Learning-with-R-datasets-master/credit.csv', stringsAsFactors = FALSE)
head(credit, 5)
```

```{r}
str(credit)
```

```{r}
table(credit$checking_balance)
```

```{r}
table(credit$savings_balance)
```

```{r}
summary(credit$amount)
```

```{r}
summary(credit$months_loan_duration)
```

```{r}
table(credit$default)
```

```{r}
set.seed(123)
train_sample <- sample(1000, 900)

credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
```

```{r}
round(prop.table(table(credit_train$default))*100, 1)
```

```{r}
defaults <- function(x){
  x <- ifelse(x==1, 'Yes', 'No')
}
```


```{r}
# CONVRTED THE DEFAULT TO FACTORS BY APPLYING THE ABOVE FUNCTION
# SINCE LAPPLY RETURNS A LIST, IT IS BEST TO USE DATA.FRAME BEFORE THE FUNCTION
credit_train <- data.frame(credit_train[1:16], lapply(credit_train[17], defaults))
credit_test <- data.frame(credit_test[1:16], lapply(credit_test[17], defaults))
str(credit_train)
table(credit_train$default)
```


```{r}
credit_train[-17] # this includes all the predictor varibales except the column 17
```


```{r}
credit_model <- C5.0(credit_train[-17], credit_train$default)
```

```{r}
credit_pred <- predict(credit_model, credit_test)
```

```{r}
CrossTable(credit_test$default, credit_pred,
             prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
             dnn = c('actual default', 'predicted default'))
```

```{r}
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c('predicted', 'actual')
matrix_dimensions
```

```{r}
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost_predict10 <- predict(credit_boost10, credit_test)

CrossTable(credit_test$default, credit_boost_predict10,
             prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
             dnn = c('actual default', 'predicted default'))
```

# SVM Classifiers

```{r}
letters <- read.csv('/Users/wafic/Downloads/Machine-Learning-with-R-datasets-master/letterdata.csv')
str(letters)
```

```{r}
letters_train <- letters[1:16000,]
letters_test <- letters[16001:20000,]
```

```{r}
letter_classifier <- ksvm(letter~., data=letters_train, kernel='vanilladot')
```

```{r}
letters_predictor <- predict(letter_classifier, letters_test)
table(letters_predictor, letters_test$letter)
```

```{r}
agreement <- letters_predictor==letters_test$letter
table(agreement)
```

```{r}
letter_classifier_rbf <- ksvm(letter~., data=letters_train, kernel='rbfdot')
letter_predictor_rbf <- predict(letter_classifier_rbf, letters_test)
agreement_rbf <- letter_predictor_rbf==letters_test$letter
table(agreement_rbf)
```

# APRIARI ALGORITHM FOR MARKET BASKET ANALYSIS

```{r}
# Import in a sparce format
groceries <- read.transactions('/Users/wafic/Downloads/Machine-Learning-with-R-datasets-master/groceries.csv', sep=',')
```

```{r}
summary(groceries)
```

```{r}
inspect(groceries[1:3])
```

```{r}
# A column of the data
itemFrequency(groceries[,1:3])
```

```{r}
# 8 items with 10% support
itemFrequencyPlot(groceries, support=0.1)
```

```{r}
# Top 20 items
itemFrequencyPlot(groceries, topN=20)
```

```{r}
# Entire Sparce Matric
image(groceries[1:100])
```

```{r}
groceryrules <- apriori(groceries, parameter = list(support =0.006, confidence = 0.25, minlen = 2))
```
```{r}
# Our algorithm has 463 rules
groceryrules
```
```{r}
# 150 rules has 2 items, 297 rules has 3 items...
summary(groceryrules)
```

```{r}
inspect(groceryrules[1:3])
```

```{r}
inspect(sort(groceryrules, by = "lift")[1:5])
```

```{r}
berryrules <- subset(groceryrules, items %in% 'berries')
inspect(berryrules)
```



> The subset() function is very powerful. The criteria for choosing the subset can be defined with several keywords and operators:
> The keyword items explained previously, matches an item appearing anywhere in the rule. To limit the subset to where the match occurs only on the left- or right-hand side, use lhs and rhs instead.
> The operator %in% means that at least one of the items must be found in the list you defined. If you want any rules matching either berries or yogurt, you could write items %in%c("berries", "yogurt”).
> Additional operators are available for partial matching (%pin%) and complete matching (%ain%). Partial matching allows you to find both citrus fruit and tropical fruit using one search: items %pin% "fruit". Complete matching requires that all the listed items are present. For instance, items %ain% c("berries", "yogurt") finds only rules with both berries and yogurt.
> Subsets can also be limited by support, confidence, or lift. For instance, confidence > 0.50 would limit you to the rules with confidence greater than 50 percent.
> Matching criteria can be combined with the standard R logical operators such as and (&), or (|), and not (!).

# K-MEANS CLUSTERING

```{r}
teens <- read.csv('/Users/wafic/Downloads/Machine-Learning-with-R-datasets-master/snsdata.csv', stringsAsFactors = FALSE)
```

```{r}
str(teens)
```

```{r}
table(teens$gender, useNA = 'ifany')
```

```{r}
summary(teens$age)
# Age 106 and 3 lol
```

```{r}
# Fixing Age
teens$age <- ifelse(teens$age >= 13 & teens$age < 20, teens$age, NA)
summary(teens$age)
```

```{r}
# Creating dummy variables for gender
teens$female <- ifelse(teens$gender == 'F' & !is.na(teens$gender), 1, 0)
teens$no_gender <- ifelse(is.na(teens$gender), 1, 0)
table(teens$no_gender, useNA = 'ifany')
```


```{r}
# Imputing missing points 5523 for age
mean(teens$age, na.rm = T)
```

```{r}
# Mean age for every graduation year
aggregate(data=teens, age~gradyear, mean, na.rm=T)
```

```{r}
# we can use the ave() function, which returns a vector with the group means repeated 
# so that the result is equal in length to the original vector:

ave_age <- ave(teens$age, teens$gradyear, FUN = function(x) mean(x, na.rm = T))

teens$age <- ifelse(is.na(teens$age), ave_age, teens$age)
summary(teens$age)
```

```{r}
interests <- teens[5:40]
```


```{r}
# rescaling to z score
interests_z <- as.data.frame(lapply(interests, scale))

# 5 Clusters
teens_cluster <- kmeans(interests_z, 5)
teens_cluster$size
```

```{r}
teens_cluster$centers
```

```{r}
teens$cluster <- teens_cluster$cluster
teens[1:5, c("cluster", "gender", "age", "friends")]
```

```{r}
aggregate(data = teens, age~cluster, mean)
```
```{r}
aggregate(data = teens, female~cluster, mean)
```
```{r}
aggregate(data = teens, friends~cluster, mean)
```

# Model Evaluation

```{r}
sms_test_prob <- predict(sms_classifier, sms_test, type = "raw")
head(sms_test_prob)
```

```{r}
sms_results <- read.csv("/Users/wafic/Downloads/Machine-Learning-with-R-datasets-master/mlwr-master/10 - Model Performance/sms_results.csv")
head(sms_results)
```

```{r}
head(subset(sms_results, actual_type!=predict_type))
```


```{r}
table(sms_results$actual_type, sms_results$predict_type)
```

```{r}
CrossTable(sms_results$actual_type, sms_results$predict_type, prop.chisq = FALSE)
```

```{r}
confusionMatrix(sms_results$actual_type, sms_results$predict_type, positive = 'spam')

# Kappa Evaluation
# • Poor agreement = less than 0.20
# • Fair agreement = 0.20 to 0.40
# • Moderate agreement = 0.40 to 0.60
# • Good agreement = 0.60 to 0.80
# • Very good agreement = 0.80 to 1.00
```

```{r}
head(sms_results)
```

```{r}
pred <- prediction(predictions=sms_results$prob_spam, labels=sms_results$actual_type)

perf <- performance(pred, measure = "tpr", x.measure = "fpr")

plot(perf, main = "ROC curve for SMS spam filter", col = "blue", lwd = 3)
```

```{r}
# Cross Validation
set.seed(123)

folds <- createFolds(credit$default, k=10)
str(folds)
credit$default <- factor(credit$default)
```

```{r}
credit01_test <- credit[folds$Fold01,]
credit01_train <- credit[-folds$Fold01,]
```


```{r}
cv_results <- lapply(folds, function(x) {
    credit_train <- credit[-x, ]
    credit_test <- credit[x, ]
    credit_model <- C5.0(default ~ ., data = credit_train)
    credit_pred <- predict(credit_model, credit_test)
    credit_actual <- credit_test$default
    kappa <- kappa2(data.frame(credit_actual, credit_pred))$value
    return(kappa)
  })

str(cv_results)
mean(unlist(cv_results))
```

# Improving Model Performance

```{r}
# Knowing tuning parameters of every model in R
modelLookup("C5.0")
```

```{r}
set.seed(300)
m <- train(default ~ ., data = credit, method = "C5.0")
```

```{r}
m
```

```{r}
p <- predict(m, credit)
table(p, credit$default)
```

```{r}
# Automatic Parameter Tuning in R
ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")

grid <- expand.grid(.model = "tree",
                      .trials = c(1, 5, 10, 15, 20, 25, 30, 35),
                      .winnow = "FALSE")

set.seed(300)
m <- train(default ~ ., data = credit, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)
```


```{r}
# Ensemble Bagging
set.seed(300)
mybag <- bagging(default ~ ., data = credit, nbagg = 25)

credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)
```

```{r}
# Bagging with 10 fold cross validation
set.seed(300)
ctrl <- trainControl(method = "cv", number = 10)
# bagged trees function is treebag
train(default ~ ., data = credit, method = "treebag",
         trControl = ctrl)
```
```{r}

# The caret package also includes example objects for bags of naive Bayes models (nbBag), decision trees (ctreeBag), and neural networks (nnetBag).


# Bagging with SVM
bagctrl <- bagControl(fit = svmBag$fit,
                        predict = svmBag$pred,
                        aggregate = svmBag$aggregate)

set.seed(300)
svmbag <- train(default ~ ., data = credit, "bag", trControl = ctrl, bagControl = bagctrl)

svmbag
```

```{r}
# ADA Boosting Algorithm

set.seed(300)
m_adaboost <- boosting(default ~ ., data = credit)
p_adaboost <- predict(m_adaboost, credit)

p_adaboost$confusion
```
```{r}
# For a more accurate assessment of performance on unseen data, we need to use another evaluation method
# Using 10 folds
adaboost_cv <- boosting.cv(default ~ ., data = credit)
adaboost_cv$confusion
```

```{r}
convert_counts <- function(x){
  x <- ifelse(x == 1, 'Yes', 'No')
}
```

```{r}
# Random Forest
set.seed(300)

credit[sapply(credit, is.numeric)] <- lapply(credit[sapply(credit, is.numeric)], as.factor)

credit[sapply(credit, is.character)] <- lapply(credit[sapply(credit, is.character)], as.factor)

#credit <- data.frame(credit[1:16] , lapply(credit[17], convert_counts))
credit_fact
rf <- randomForest(default ~ ., data = credit[-c(2, 5, 13)])
rf
```

```{r}
m_rf <- train(default ~ ., data = credit, method = "rf",
                metric = "Kappa", trControl = ctrl,
                tuneGrid = grid_rf)
```


