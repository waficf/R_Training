# Housing Prices Submission
***

## Importing the needed libraries
```{r eval=FALSE, Packages}
library(dplyr)
library(ggplot2) # Data visualization
library(VIM)  # Plots missing data
library(mice)
library(ggthemes)
library(scales) # Scaling yaxis
library(caret)
library(ggcorrplot)
library(grid)
library(gridExtra)
library(corrplot)
library(knitr) # Convert to html
library(rpart) # Fancy decisionmaking plots
library(rpart.plot) # Draw tree
library(GGally)
library(factoextra) # PCA Cluster
library("clustertend") # Checks whether data set has tendency for clustering
```


## Data Exploration
```{r}
wholesale <- read.csv('/Users/wafic/Downloads/data/Unsupervised data/Wholesale_customers_data.csv')
```

```{r}
str(wholesale)
```

```{r}
head(wholesale)
```

#### Implementation: Selecting Samples

For the purposes of this project, the features 'Channel' and 'Region' will be 
excluded in the analysis â€” with focus instead on the six product categories 
recorded for customers

```{r}
data <- wholesale[3:8]
data
```

```{r}
summary(data)
```

```{r}
ggplot(stack(data), aes(x=ind, y = values))+
  geom_boxplot()
```


```{r}
data[c(67, 155, 339, 334),]
```

Question 1
Consider the total purchase cost of each product category and the statistical 
description of the dataset above for your sample customers.
What kind of establishment (customer) could each of the three samples you've 
chosen represent?

67: Cafe
I think it is a cafe because it is sure not a restaurant or a house as they are
very low Fresh & Frozen but for groceries and Detergents_Paper they have average 
spending which can be justified due to cleaning bathrooms and tables

155: House
The fact that the spending is not on the first Quartile in any of the products
makes it normal for a house where you buy lots of Fresh products but not in the
thousands, same applies to the rest

339: Retail Store
With minimal spending on Fresh, Milk and Detergents_Paper along with huge 
Frozen and Grocery purchases, it sure not a house nor a restautant or a hotel.

334: Hotel or Restaurant
The huge spending on Detergents_Paper and Grocery makes it very unlikely to be 
house but sure a business but unlikey to be a retailer


#### Implementation: Feature Relevance
One interesting thought to consider is if one (or more) of the six product 
categories is actually relevant for understanding customer purchasing. 
That is to say, is it possible to determine whether customers purchasing some 
amount of one category of products will necessarily purchase some proportional 
amount of another category of products? We can make this determination quite 
easily by training a supervised regression learner on a subset of the data with 
one feature removed, and then score how well that model can predict the removed 
feature.

```{r}
set.seed(300)
split = 0.75

features <- names(data)

for (i in features){
  trainIndex <- createDataPartition(data[,i], p=split, list=FALSE)
  
  train <- data[trainIndex,]
  train_new <- subset(train, select = -c(data[,i]) )
  test <- data[-trainIndex,]
  
  set.seed(3000)
  tuneGrid <- expand.grid(cp = c(0.01, 0.05))
  
  rpart_fit <- train(x = train_new, y = train[,i], 
                     method = "rpart",
                     tuneGrid = tuneGrid,
                     control = rpart.control(minsplit = 10, minbucket = 5))
  
  print(paste('Predictor is: ', i))
  predict = predict(rpart_fit, test)
  print(postResample(pred = predict, obs = test[,i]))
}
```



```{r echo = FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.width=7, fig.height=4}
ggpairs(data=data,
        upper = list(continuous = "cor"),
        lower = list(continuous = "points"),
        title="Wholesale Features")
```

```{r}
log_data <- log10(data)
```

```{r echo = FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.width=7, fig.height=4}
ggpairs(data=log_data,
        upper = list(continuous = "cor"),
        lower = list(continuous = "points"),
        title="Wholesale Features")
```

```{r}
head(log_data)
```

#### Outliers
```{r}
for (i in features){
  q1 <- quantile(log_data[,i], 0.25)
  q3 <- quantile(log_data[,i], 0.75)
  
  outlier_step <- 1.5 * (q3-q1)
  print(log_data[(log_data[,i] < (q1 - outlier_step)) | (log_data$Fresh > (q3 + outlier_step)),])
  
}
```

```{r}
ggplot(data = stack(log_data), aes(x=ind, y = values))+
  geom_boxplot()
```

## PCA 

```{r}
pca <- PCA(log_data, graph = FALSE, ncp = 6)
pca$var$coord
```


```{r}
eig.val <- get_eigenvalue(pca)
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))
```
Dimensions 1 and 2 explain almost 70% of the variation in the data

```{r}
fviz_pca_var(pca, col.var = "black")
```
> Dimension 1: Milk, Grocery, Detergents_Paper
> Dimension 2: Fresh, Frozen, Delicassen

```{r}
as.data.frame(pca$ind)[1:4,][,1:6]
```

In the code block below, you will need to implement the following:
Assign the results of fitting PCA in two dimensions with good_data to pca.
Apply a PCA transformation of good_data using pca.transform, and assign the reuslts to reduced_data.
Apply a PCA transformation of the sample log-data log_samples using pca.transform, and assign the results to pca_samples.
```{r}
pca_2 <- PCA(log_data, graph = FALSE, ncp =2)
as.data.frame(pca_2$ind)[1:3,][,1:2]
```

```{r}
pca_2$var$contrib
```

```{r}
# Contributions of variables to PC1
fviz_contrib(pca_2, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(pca_2, choice = "var", axes = 2, top = 10)
```

## K-means Clustering

```{r}
# Converting values into dataframe to use PC1 and PC2 in clustering algorithm
pca_data <- as.data.frame(pca_2$ind)[,1:2]
```

```{r}
fviz_nbclust(pca_data, kmeans, method = "silhouette")+
theme_classic()
```

```{r}
set.seed(123)
km.res <- kmeans(pca_data, 2, nstart = 25)
fviz_cluster(km.res, data = pca_data,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())
```

