# Titanic Machine Leaning First Submission
***
This is my first Kaggle submission, I really learned a lot from practicing
here than from reading several books. You can see that I have taken lots of 
chunks of code from Megan Risdal submission and I tried to run every algorithm
I learned so far for the sake of practice

## 1. Importing the needed libraries
```{r eval=FALSE, Packages}
library(dplyr)
library(ggplot2) # Data visualization
library(GGally) #ggpair
library(ElemStatLearn) # KNN classifier plot
library(rpart) # Fancy decision making plots
library(rpart.plot) # Draw tree
library(readr) # CSV file I/O, e.g. the read_csv function
library(tm) # text mining in R
library(knitr) # Convert to html
library(SnowballC) # Removes stemming words
library(wordcloud)
library(tidyverse)
library(gmodels) # Uses crosstab function
library('mice') # imputation
library(VIM)  # Plots missing data
library(gridExtra) #Plotting ggplots side by side
library(ggpubr) # Nice Theme
library(C50) # Package that includes the decision tree algorithm
library(e1071) # Naive Bayes package (naiveBayes)
library(klaR) # Another naive bayes package (NaiveBayes)
library('randomForest') # Random Forest
library(caret) # Splitting data including cross validation
library(class) # knn Classifier
library(randomForest) # RANDOM FOREST
library(varhandle)
```

## 2. Importing Data with Little EDA
```{r}
titanic <- read.csv('/Users/wafic/Downloads/data/titanic_data.csv', stringsAsFactors = FALSE, na.strings=c("","NA"))
```

```{r}
dim(titanic)
```

```{r}
str(titanic)
```
We can see that some variables like PassengerId and Ticket are unique to every 
passenger and hold little value for our algorithm

```{r}
nulls <- function(x){
  sum(is.na(x))
}

sapply(titanic, nulls)
```
We've got several variables with missing data points, usually we can drop a 
variable like 'Cabin' since most of the data is missing but we can impute Age
and Embarked

```{r}
data_tit <- subset(titanic, select=-c(Ticket, PassengerId, Cabin))
str(data_tit)
```
Dropped Ticket, PassengerId and Cabin from our variables into a new dataframe

```{r}
data_tit$Survived <- factor(data_tit$Survived)
prop.table(table(data_tit$Survived))*100
```

```{r}
prop.table(table(data_tit$Embarked))*100
```

```{r}
prop.table(table(data_tit$Sex))*100
```

```{r}
summary(data_tit$Fare)
```
With median Fare of 14.45, some members paid 512 which is quite expensive

```{r}
summary(data_tit$Age)
```
Average age is around 30 which I believe is due to the european migration
to the US at the start of the 20th century in pusuit of a better life at the new
land (later known as the American Dream, term coined din 1931)

```{r}
ggplot(aes(x=Fare), data=data_tit)+
  geom_histogram()
```
We can see only one member (outlier) whom paid above 500

```{r}
ggplot(aes(x=Age), data = data_tit)+
  geom_histogram()
```
A bit of a right skewed graph as the majority of travelers are between 20 and 40
with very few old members above 60. We can see quite some infants and youngsters
below 20 which implies that there where some entire families migrating

## 3. Filling up the missing data points in Embarked and Age

```{r}
aggr_plot <- aggr(data_tit, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data_tit), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```
Lots of points missing in Age and few in Embarked

#### 3.1 Imputing Age

```{r}
Pmiss <- function(x){
  sum(is.na(x))/length(x)*100
}
Pmiss(data_tit$Age)
```
Almost 20% of Age is missing, its better to impute than drop the entire variable

```{r}
tempdata <- mice(data_tit, m=5, maxit=50, meth='pmm',seed=500)
summary(tempdata)
```


```{r}
mice_output <- complete(tempdata, 1)
```

```{r}
par(mfrow=c(1,2))
hist(data_tit$Age, freq=F, main='Age: Original Data', 
  col='darkgreen', ylim=c(0,0.04))
hist(mice_output$Age, freq=F, main='Age: MICE Output', 
  col='lightgreen', ylim=c(0,0.04))
```

```{r}
data_tit$Age <- mice_output$Age
sum(is.na(data_tit$Age))
```


#### 3.2 Imputing Embarked
```{r}
which(is.na(data_tit$Embarked), arr.ind=TRUE)
```

```{r}
ggplot(aes(x=Age, y=Fare, colour = factor(Survived), shape=Sex), data=data_tit)+
  geom_point(alpha = 0.8)+
  facet_grid(Pclass~Embarked)+
  scale_y_log10()
```
The 2 points are survivars and females whom Fare is a around 100

```{r}
data_tit[830,]$Fare;data_tit[62,]$Fare
```


```{r}
data_embark <- data_tit[-c(62, 830),]

median(data_embark[data_embark$Pclass == 1 & data_embark$Embarked == 'C',]$Fare);median(data_embark[data_embark$Pclass == 1 & data_embark$Embarked == 'S',]$Fare)
```
Since they both paid 80 and the median Fare for C is 78 and S is 52, we better 
put them in Embarkment C

```{r}
ggplot(aes(x=Embarked, y=Fare, fill=factor(Pclass)), data=data_embark)+
  geom_boxplot()+
  geom_hline(aes(yintercept=80),colour='red', linetype='dashed', lwd=2)
```
The red line is the Fare they paid and we can see it is on the median of C 
but the end of the IQR plot in S

```{r}
data_tit$Embarked[c(62, 830)] <- 'C'
```


```{r}
sapply(data_tit, nulls)
```
We have no missing data points anymore


## 4. Feature Engineering

#### 4.1 Altering the Title Variable
```{r}
data_tit$Title <- gsub("(.*, )|(\\..*)", '',data_tit$Name)
table(data_tit$Title, data_tit$Sex)
```
I would never thought of extracting data from the Title variable but after 
reading Megan work I did and we can see some really valuable stufff here

```{r}
rare_titles <- c("Capt", "Col", "Don", "Dr", "Jonkheer", "Lady", "Major", "Rev", "Sir", "the Countess")
data_tit$Title[data_tit$Title == 'Mme'] <- 'Mrs'
data_tit$Title[data_tit$Title == 'Mlle'] <- 'Miss'
data_tit$Title[data_tit$Title == 'Ms'] <- 'Miss'
data_tit$Title[data_tit$Title %in% rare_titles] <- 'Rare'
table(data_tit$Title, data_tit$Sex)
```


```{r}
ggplot(aes(x=Title, fill=factor(Survived)), data=data_tit)+
  geom_bar()+
  ggtitle("Plotting of the New Variable Title")+
  theme_classic()
```

#### 4.2 Traveling Families vs Singles

```{r}
data_tit$F_Size <-  data_tit$SibSp + data_tit$Parch +1
table(data_tit$F_Size)
```
More than half of travelers are singles, I bet most are single young men in
their thirtees

```{r}
ggplot(aes(x=F_Size, fill=factor(Survived)),data=data_tit)+
  geom_bar(position = "dodge")+
  xlab("Family Size")+
  theme_classic()+
  ggtitle('Family Size')
```
Its better to travel in packs rather than alone

```{r}
data_tit$F_Size_D[data_tit$F_Size == 1] <- 'Singles'
data_tit$F_Size_D[data_tit$F_Size > 1 & data_tit$F_Size < 5] <- 'Small'
data_tit$F_Size_D[data_tit$F_Size >= 5] <- 'Large'
table(data_tit$F_Size_D, data_tit$Survived)
```
Of the 537 singles, 374 perished

```{r}
mosaicplot(table(data_tit$F_Size_D, data_tit$Survived), main = "Family Survival on the Titanic", color = TRUE, shade=TRUE)
```


#### 4.3 Creating a new varibale Child/Adult form Age

```{r}
data_tit$Child[data_tit$Age < 18] <- 'Child'
data_tit$Child[data_tit$Age >= 18] <- 'Adult'
table(data_tit$Child, data_tit$Survived)
```

```{r}
ggplot(aes(x=Age, fill=factor(Survived)), data=data_tit)+
  geom_histogram()+
  facet_grid(.~Child)
```

#### 4.3 Creating a new variable Mother form Age and Family 
```{r}
data_tit$Mother <- 'Not Mother'
data_tit$Mother[data_tit$Child == 'Adult' & data_tit$Sex == 'female' & data_tit$Parch > 0 & data_tit$Title != 'Miss'] <- 'Mother'

table(data_tit$Mother, data_tit$Survived)
```

```{r}
data_tit$Child <- factor(data_tit$Child)
data_tit$Mother <- factor(data_tit$Mother)
```


```{r}
data_tit$Pclass <- factor(data_tit$Pclass)
data_tit$Sex <- factor(data_tit$Sex)
data_tit$Embarked <- factor(data_tit$Embarked)
data_tit$Title <- factor(data_tit$Title)
data_tit$F_Size_D <- factor(data_tit$F_Size_D)
str(data_tit)
```
This is our new dataframe with new variable but we have variables that are
useless and can be dropped

```{r}
full_data <- data_tit[-3]
full_data <- full_data[-10]

convert_counts <- function(x){
  x <- ifelse(x == 1, 'Yes', 'No')
}

x <- data.frame(lapply(full_data[1], convert_counts))
full_data[1]<- x
full_data$Survived <- factor(full_data$Survived)

str(full_data)
```
Dropped variables Name as it has no value in predicting survivars and the 
variable F_size as it is changes to F_Size_D

## 5. Predictive Models

#### 5.1 K-NN Classifier
```{r}

# Normalizing the Fare and Age variables
normalize<- function(x){
  return((x-min(x))/(max(x)-min(x)))
}

normalized_data <- full_data[c(4, 7)]
normalized_data <- data.frame(lapply(normalized_data, normalize))
full_data_norm <- full_data

full_data_norm[c(4, 7)] <- normalized_data
str(full_data_norm)

```

```{r}
set.seed(300)
split = 0.8
trainIndex <- createDataPartition(full_data_norm$Survived, p=split, list=FALSE)
norm_train <- full_data_norm[trainIndex,]
norm_test <- full_data_norm[-trainIndex,]

norm_train_labels <- full_data_norm[trainIndex,]$Survived
norm_test_labels <- full_data_norm[-trainIndex,]$Survived
```

```{r}
ctr <- trainControl(method="repeatedcv",number =10 ,repeats = 3)

knn_classifier <- train(Survived ~ ., data = norm_train, method = "knn", trControl = ctr, preProcess = c("center","scale"), tuneLength = 20, na.action="na.omit")

knn_classifier
```

```{r}
plot(knn_classifier)
```


```{r}
knn_predict <- predict(knn_classifier, newdata = norm_test)
knn_table <- table(actualclass=norm_test_labels, predictedclass=knn_predict)
confusionMatrix(knn_table)
```

```{r}
CrossTable(x = norm_test_labels, y = knn_predict,prop.chisq=FALSE,dnn = c('Actual', 'Predicted')) 
```

#### 5.2 Naive Bayes Classifier
```{r}
set.seed(300)
norm_train_p <- norm_train[2:12]
norm_test_p <- norm_test[2:12]

naive_classifier <- naiveBayes(norm_train_p, norm_train_labels)
naive_predictor <- predict(naive_classifier, norm_test_p)

nb_table <- table(actualclass=norm_test_labels, predictedclass=naive_predictor)
confusionMatrix(nb_table)
```


```{r}
CrossTable(x = norm_test_labels, y = naive_predictor, prop.chisq=FALSE,dnn = c('Actual', 'Predicted')) 
```
#### 5.3.1 Decision Tree (Vanilla Version)
```{r}
tree_classifier <- C5.0(norm_train[-1], norm_train$Survived)
tree_predictor <- predict(tree_classifier, norm_test)

tree_table <- table(actualclass=norm_test_labels, predictedclass=tree_predictor)
confusionMatrix(tree_table)
```


```{r}
CrossTable(x=norm_test_labels, y=tree_predictor,
           prop.chisq = FALSE, prop.t = FALSE,
           dnn = c('actual', 'predicted'))
```

#### 5.3.2 Decision Tree (Auto Paramter Tuning)
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
grid_rf <- expand.grid(.mtry = c(2, 4))

grid_c50 <- expand.grid(.model = "tree",
                          .trials = c(10, 20),
                          .winnow = "FALSE")
set.seed(300)
m_c50 <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + Child + Mother, 
              data = train, method = "C5.0",
              metric = "Kappa", trControl = ctrl, tuneGrid = grid_c50)


model.tree  <- train(Survived ~ .,
                   data = norm_train,
                   method = "C5.0",
                   trControl = ctrl, 
                   tuneGrid = grid_c50)

plot(varImp(model.tree), top = 10)
```

#### 5.3.3 Decision Tree (Classification and regression trees via Rpart)
```{r}
# https://www.statmethods.net/advstats/cart.html
tree_fit <- rpart(Survived ~ .,
              data = norm_train,
              method = "class")
rsq.rpart(tree_fit)
```

```{r}
rpart.plot(tree_fit)
```


#### 5.4.1 Random Forest (Vanilla Version)
```{r}
rf <- randomForest(Survived ~ ., data = norm_train)


print(rf) # view results 
importance(rf) # importance of each predictor
```

#### 5.4.2 Random Forest (Auto Paramter Tuning)
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
grid_rf <- expand.grid(.mtry = c(2, 4))

set.seed(300)
m_rf <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + Child + Mother, 
              data = train, method = "rf",
              metric = "Kappa", trControl = ctrl, tuneGrid = grid_rf)
m_rf
```


